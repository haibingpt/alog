//
//  Transcription.swift
//  ALog
//
//  Created by Xin Du on 2023/07/15.
//

import Foundation
import XLog
import Combine

enum TranscriptionError: LocalizedError {
    case invalidCustomServer
    var errorDescription: String? {
        switch self {
        case .invalidCustomServer: return L(.error_invalid_custom_server)
        }
    }
}

class Transcription {
    
    static let shared = Transcription()
    
    private let TAG = "Trans"
    
    var memoQueue = [(MemoEntity, (Result<String, Error>) -> Void)]()
    var activeTask = 0
    
    /// ËΩ¨ÂÜôÊúÄÂ§ßÂπ∂Âèë
    var maxConcurrent: Int {
        if Config.shared.transProvider == .apple {
            return 1
        }
        return Config.shared.serverType == .app ? 2 : 4
    }
    
    /// Âª∂Ëøü
    var delay: UInt64 {
        if Config.shared.transProvider == .apple {
            return 1
        }
        return Config.shared.serverType == .app ? 1 : 0
    }
    
    let hallucinationList: Set<String> = [
        "ËØ∑‰∏çÂêùÁÇπËµû ËÆ¢ÈòÖ ËΩ¨Âèë ÊâìËµèÊîØÊåÅÊòéÈïú‰∏éÁÇπÁÇπÊ†èÁõÆ",
        "Ë´ã‰∏çÂêùÈªûË¥äË®ÇÈñ±ËΩâÁôºÊâìË≥ûÊîØÊåÅÊòéÈè°ËàáÈªûÈªûÊ¨ÑÁõÆ",
        "Â≠óÂπïÁî±Amara.orgÁ§æÂå∫Êèê‰æõ",
        "Â≠óÂπïÁî±Amara.orgÁ§æÂå∫Êèê‰æõ Â≠óÂπïÁî±Amara.orgÁ§æÂå∫Êèê‰æõ",
        "Â∞èÁ∑®Â≠óÂπïÁî±Amara.orgÁ§æÂçÄÊèê‰æõ",
        "Â≠óÂπïbyÁ¥¢ÂÖ∞Â®Ö",
        "Áî± Amara.org Á§æÁæ§Êèê‰æõÁöÑÂ≠óÂπï"
    ]
    
    func transcribe(voiceURL: URL, provider: TranscriptionProvider, lang: TranscriptionLang) async throws -> String {
        XLog.info("Transcribe \(voiceURL.lastPathComponent) using \(provider). lang = \(lang.rawValue)", source: TAG)
        if provider == .apple {
            let text = try await SpeechRecognizer.shared.transcribe(voiceURL, lang: lang)
            return text
        } else if provider == .openai {
            if Config.shared.serverType == .custom && !Config.shared.isServerSet {
                throw TranscriptionError.invalidCustomServer
            }
            
            let model = Config.shared.transModel
            let text = try await OpenAIClient.shared.transcribe(voiceURL, lang: lang, model: model.name).text
            if hallucinationList.contains(text) {
                XLog.info("üòµ‚Äçüí´ skip '\(text)'", source: TAG)
                return ""
            }
            return text
        }
        return ""
    }
    
    func transcribe(_ memo: MemoEntity, completion: @escaping (Result<String, Error>) -> Void) {
        memoQueue.append((memo, completion))
        processNext()
    }
    
    private func processNext() {
        while !memoQueue.isEmpty && activeTask < maxConcurrent {
            activeTask += 1
            let (memo, completion) = memoQueue.removeFirst()
            
            Task { @MainActor in
                do {
                    try await Task.sleep(nanoseconds: delay * 1_000_000_000)
                    let voiceURL = FileHelper.fullAudioURL(for: memo.file!)
                    let text = try await transcribe(voiceURL: voiceURL, provider: Config.shared.transProvider, lang: Config.shared.transLang)
                    completion(.success(text))
                } catch {
                    completion(.failure(error))
                }
                activeTask -= 1
                processNext()
            }
        }
    }
}
